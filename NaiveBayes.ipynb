{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "417a013f",
   "metadata": {},
   "source": [
    "# Building a Naïve Bayes Spam Filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a9ad967e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import string\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0444637d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the four dataset files into Python data frames and the two list files into lists\n",
    "def loadcsv(filepath):\n",
    "    data_raw = pd.read_csv(filepath, header=0, usecols=[0,1], encoding=\"latin1\")\n",
    "    data_raw.columns = ['label', 'sms']\n",
    "    return data_raw\n",
    "\n",
    "def loadcensored(filepath):\n",
    "    with open(filepath, 'r') as f:\n",
    "        return f.read().splitlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6428aae0",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = loadcsv('training.csv')\n",
    "valid = loadcsv('validation.csv')\n",
    "test1 = loadcsv('test1.csv')\n",
    "test2 = loadcsv('test2.csv')\n",
    "\n",
    "cens1 = loadcensored('censored_list_test1.txt')\n",
    "cens2 = loadcensored('censored_list_test2.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a205becd",
   "metadata": {},
   "source": [
    "I will start by pre-processing the SMS messages: Remove all punctuation and numbers from the SMS messages, and change all messages to lower case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "bd52909e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pre_process(df):\n",
    "    whitelist = set('abcdefghijklmnopqrstuvwxyz ')\n",
    "    df['sms'] = df.sms.apply(lambda x: ''.join(filter(whitelist.__contains__, x.lower())))\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69c7ed35",
   "metadata": {},
   "source": [
    "Before appying the pre-process function, I check the original data first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "dcf0cdba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  label                                                sms\n",
      "0   ham  Just sent again. Do you scream and moan in bed...\n",
      "1   ham            When i have stuff to sell i.ll tell you\n",
      "2   ham                   Ugh fuck it I'm resubbing to eve\n",
      "3   ham                      Change windows logoff sound..\n",
      "4   ham             Ití_ís í«£6 to get in, is that ok?\n"
     ]
    }
   ],
   "source": [
    "# show the first 5 rows of the training data\n",
    "print(train.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ac84076",
   "metadata": {},
   "source": [
    "Now I adopt the pre-process function to the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ebe9a452",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  label                                                sms\n",
      "0   ham  just sent again do you scream and moan in bed ...\n",
      "1   ham             when i have stuff to sell ill tell you\n",
      "2   ham                    ugh fuck it im resubbing to eve\n",
      "3   ham                        change windows logoff sound\n",
      "4   ham                          its  to get in is that ok\n"
     ]
    }
   ],
   "source": [
    "# pre-process the training data\n",
    "train = pre_process(train)\n",
    "# show the first 5 rows of the training data\n",
    "print(train.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24550e19",
   "metadata": {},
   "source": [
    "Similarly, I pre-process the other three data files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "983b09bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "valid = pre_process(valid)\n",
    "test1 = pre_process(test1)\n",
    "test2 = pre_process(test2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa414757",
   "metadata": {},
   "source": [
    "The functions 'train' and 'train2' calculate and store the prior probabilities and likelihoods. In Naive Bayes this is all the training phase does.\n",
    "\n",
    "The 'predict' function repeatedly applies Bayes' Theorem to every word in the constructed dictionary, and based on the posterior probability it classifies the message as 'spam' or 'ham'.\n",
    "\n",
    "The 'score' function calls 'predict' for multiple messages and compares the outcomes with the supplied 'ground truth' labels and thus evaluates the classifier. It also computes and returns a confusion matrix.\n",
    "\n",
    "The difference between 'train' and 'train2' is that the latter function only takes into account words that are 20 times more likely to appear in a spam message than a ham message. Not all words are equally informative. Using 'train2' will decrease the size of the dictionary significantly, thus making the classifier more efficient. There are multiple other ideas that one can use to construct more informative dictionaries. For example, you can treat words with the same root ('go', 'goes', 'went', 'gone', 'going') as the same word."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11c3c44e",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d46e9da1",
   "metadata": {},
   "source": [
    "Now, I will use my training set to train the classifiers 'train' and 'train2'. The training functions require the ham and spam messages to be passed on separately:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6d05fedf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1826\n",
      "275\n"
     ]
    }
   ],
   "source": [
    "hammsgs = train[train['label'] == 'ham']['sms'].tolist()\n",
    "spammsgs = train[train['label'] == 'spam']['sms'].tolist()\n",
    "print(len(hammsgs))\n",
    "print(len(spammsgs))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b937d9b6",
   "metadata": {},
   "source": [
    "We have 1826 ham messages and 275 spam messages in our training set. Now we can create a classifier and train it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4dabd3f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#define a class \n",
    "class NaiveBayesForSpam: \n",
    "    #define a method to train the classifier \n",
    "    def train(self, hamMessages, spamMessages):\n",
    "        #create a set of unique words from ham and spam messages\n",
    "        #make all the words a long string connected with -, split this string into words, and remove duplicates\n",
    "        self.words = set('-'.join(hamMessages+spamMessages).split())\n",
    "        #assign [0., 0.] to prior probabilities of ham and spam\n",
    "        self.priors = np.zeros(2)\n",
    "        #set the first value as the prior probabilities of ham \n",
    "        # P(ham) = num of ham messages / num of all messages\n",
    "        self.priors[0] = float(len(hamMessages)/(len(spamMessages)+len(hamMessages)))\n",
    "        #set the second value as the prior probabilities of spam \n",
    "        # P(spam) = num of spam messages / num of all messages\n",
    "        self.priors[1] = 1.0 - self.priors[0]\n",
    "        #create a list to store likelihoods\n",
    "        self.likelihoods = []\n",
    "        \n",
    "        #iterate the every word in words\n",
    "        for i, w in enumerate(self.words):\n",
    "            #likelihood of the word appearing in ham messages = P(word|ham)\n",
    "            #add 1 for Laplace smoothing to avoid zero probabilities\n",
    "            prob1 = (1.0 + len([m for m in hamMessages if w in m])) / len(hamMessages)\n",
    "            #likelihood of the word appearing in spam messages = P(word|spam)\n",
    "            #add 1 for Laplace smoothing to avoid zero probabilities\n",
    "            prob2 = (1.0 + len([m for m in spamMessages if w in m])) / len(spamMessages)\n",
    "            #limit the max likelihood to be 0.95\n",
    "            #each element in likelihoods is [P(word|ham), P(word|spam)]\n",
    "            self.likelihoods.append([min(prob1, 0.95), min(prob2, 0.95)])\n",
    "            #convert list likelihoods to array, transpose so that each row corresponds to a class\n",
    "        self.likelihoods = np.array(self.likelihoods).T\n",
    "    \n",
    "    #alternative training method                 \n",
    "    def train2(self, hamMessages, spamMessages):\n",
    "        #same as train\n",
    "        self.words = set('-'.join(hamMessages+spamMessages).split())\n",
    "        self.priors = np.zeros(2)\n",
    "        self.priors[0] = float(len(hamMessages)/(len(spamMessages)+len(hamMessages)))\n",
    "        self.priors[1] = 1.0 - self.priors[0]\n",
    "        self.likelihoods = []\n",
    "        #create a list to store spam words\n",
    "        spamkeywords = []\n",
    "        \n",
    "        #interate every word in words to calculate the likelihoods of the word in ham and spam                       \n",
    "        for i, w in enumerate(self.words):\n",
    "            prob1 = (1.0 + len([m for m in hamMessages if w in m])) / len(hamMessages) #P(word|ham)\n",
    "            prob2 = (1.0 + len([m for m in spamMessages if w in m])) / len(spamMessages) #P(word|spam)\n",
    "            #add a condition\n",
    "            #if the word is much more likely to appear in spam\n",
    "            if prob1*20 <prob2:\n",
    "                self.likelihoods.append([min(prob1, 0.95), min(prob2, 0.95)])\n",
    "                spamkeywords.append(w) #append that word to spamkeywords\n",
    "        #update words to include only spamkeywords\n",
    "        self.words = spamkeywords\n",
    "        #likellihoods now include only filtered words (more likely in spam)\n",
    "        self.likelihoods = np.array(self.likelihoods).T\n",
    "    \n",
    "    #define a method to make predictions\n",
    "    def predict(self, message):\n",
    "        #set the initial posterior probailities  as prior probabilities \n",
    "        posteriors = np.copy(self.priors)\n",
    "        #iterate every word in words\n",
    "        for i, w in enumerate(self.words):\n",
    "            #if w is found in the message(case-insensitive)\n",
    "            if w in message.lower(): \n",
    "                #updated posterior = posterior*likelihood\n",
    "                posteriors*= self.likelihoods[:, i]\n",
    "            else:\n",
    "                #if w is not found in message, updated posterior = posterior*(1-likelihood)\n",
    "                posteriors*= np.ones(2) - self.likelihoods[:, i]\n",
    "            posteriors = posteriors/np.linalg.norm(posteriors, ord = 1) #normalise \n",
    "        if posteriors[0] > 0.5:\n",
    "            return ['ham', posteriors[0]] #likely to be ham\n",
    "        return ['spam', posteriors[1]] #likely to be spam\n",
    "    \n",
    "    #define a method to measure the accuracies of the classifier \n",
    "    def score(self, messages, labels):\n",
    "        #initialize a confusion matrix\n",
    "        confusion = np.zeros(4).reshape(2,2)\n",
    "        #iterate every word in messages and give scores to confusion matrix\n",
    "        for m, l in zip(messages, labels):\n",
    "            #prediction is ham, actual label is ham\n",
    "            if self.predict(m)[0] == 'ham' and l == 'ham':\n",
    "                confusion[0,0]+=1\n",
    "            #prediction is ham, actual label is spam\n",
    "            elif self.predict(m)[0] == 'ham' and l == 'spam':\n",
    "                confusion[0,1]+=1\n",
    "            #prediction is spam, actual label is ham\n",
    "            elif self.predict(m)[0] == 'spam' and l == 'ham':\n",
    "                confusion[1,0]+=1\n",
    "            #prediction is spam, actual label is spam\n",
    "            elif self.predict(m)[0] == 'spam' and l == 'spam':\n",
    "                confusion[1,1]+=1\n",
    "        #return accuracy score = num of words correctly classified / num of all words , and the confusion matrix\n",
    "        return (confusion[0,0] + confusion[1,1]) / float(confusion.sum()), confusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "03d73415",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = NaiveBayesForSpam()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "aa645e49",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf.train(hammsgs, spammsgs)\n",
    "clf.train2(hammsgs, spammsgs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe46e5fe",
   "metadata": {},
   "source": [
    "We employ both training methods to get two separate classifiers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "33bb0df9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For train function:\n",
      "In-sample accuracy is 0.96573060447406\n",
      "In-sample confusion matrix is \n",
      "[[1795.   41.]\n",
      " [  31.  234.]]\n",
      "Run time: 98.57\n"
     ]
    }
   ],
   "source": [
    "clf = NaiveBayesForSpam()\n",
    "time1 = time.time()\n",
    "clf.train(hammsgs, spammsgs)\n",
    "accuracy_train, confusion_matrix_train = clf.score(train['sms'], train['label'])\n",
    "time2 = time.time()\n",
    "print('For train function:')\n",
    "print('In-sample accuracy is {}'.format(accuracy_train))\n",
    "print('In-sample confusion matrix is \\n{}'.format(confusion_matrix_train))\n",
    "print(\"Run time: {}\".format(round(time2 - time1, 2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "91e984ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For train2 function:\n",
      "In-sample accuracy is 0.9690623512613041\n",
      "In-sample confusion matrix is \n",
      "[[1819.   58.]\n",
      " [   7.  217.]]\n",
      "Run time: 3.08\n"
     ]
    }
   ],
   "source": [
    "clf2 = NaiveBayesForSpam()\n",
    "time1 = time.time()\n",
    "clf2.train2(hammsgs, spammsgs)\n",
    "accuracy2_train, confusion_matrix2_train = clf2.score(train['sms'], train['label'])\n",
    "time2 = time.time()\n",
    "print('For train2 function:')\n",
    "print('In-sample accuracy is {}'.format(accuracy2_train))\n",
    "print('In-sample confusion matrix is \\n{}'.format(confusion_matrix2_train))\n",
    "print(\"Run time: {}\".format(round(time2 - time1, 2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7613d684",
   "metadata": {},
   "source": [
    "Using the validation set, I will explore how each of the two classifiers performs out of sample:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "404f3969",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For train function:\n",
      "Out-of-sample accuracy is 0.9655172413793104\n",
      "Out-of-sample confusion matrix is \n",
      "[[778.  21.]\n",
      " [ 10.  90.]]\n",
      "Run time: 38.08\n"
     ]
    }
   ],
   "source": [
    "time1 = time.time()\n",
    "accuracy_valid, confusion_matrix_valid = clf.score(valid['sms'], valid['label'])\n",
    "time2 = time.time()\n",
    "print('For train function:')\n",
    "print('Out-of-sample accuracy is {}'.format(accuracy_valid))\n",
    "print('Out-of-sample confusion matrix is \\n{}'.format(confusion_matrix_valid))\n",
    "print(\"Run time: {}\".format(round(time2 - time1, 2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "de5a13fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For train2 function:\n",
      "Out-of-sample accuracy is 0.9632925472747497\n",
      "Out-of-sample confusion matrix is \n",
      "[[784.  29.]\n",
      " [  4.  82.]]\n",
      "Run time: 1.02\n"
     ]
    }
   ],
   "source": [
    "time1 = time.time()\n",
    "accuracy2_valid, confusion_matrix2_valid = clf2.score(valid['sms'], valid['label'])\n",
    "time2 = time.time()\n",
    "print('For train2 function:')\n",
    "print('Out-of-sample accuracy is {}'.format(accuracy2_valid))\n",
    "print('Out-of-sample confusion matrix is \\n{}'.format(confusion_matrix2_valid))\n",
    "print(\"Run time: {}\".format(round(time2 - time1, 2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "744cee0d",
   "metadata": {},
   "source": [
    "The difference between 'train' and 'train2' is that the latter function only takes into account words that are 20 times more likely to appear in a spam message than a ham message. This makes the dictionary much smaller and hence speeds up the algorithm -> 'train2' classifier is faster\n",
    "\n",
    "The reason why a simpler model (less words) give a better performance (not just out-of-sample (in the validation set), where overfitting may deteriorate the quality of more sophisticated methods, but also in-sample (in the training set)) is the conditional independence assumption of Naive Bayes. With more words in our dictionary, the number of correlated words is larger and therefore the violation of our theoretical assumption stronger. It appears that 'train2' mediates this effect -> 'train2' classifier has a better accuracy noth on the training and validation set"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1559f9d",
   "metadata": {},
   "source": [
    "We can see from the confusion matrices that the number of FPs (ham messages classified as spam messages) for the 'train' function is 21. As for 'train2' function, the number of FPs is 29.\n",
    "\n",
    "Reduce false positives at the expense of possibly having more false negatives (spam messages classified as ham messages) -> We can achieve the trade-off between FPs and FNs by changing the following line in the 'predict' function: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06b5cab6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# if posteriors[0] > 0.5:\n",
    "#     return ['ham', posteriors[0]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "634972ea",
   "metadata": {},
   "source": [
    "By modifying the threshold from 0.5 to other values. To decrease the FP rate, we could decrease the threshold."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48afdeb3",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2363c729",
   "metadata": {},
   "source": [
    "Now, I am going to use the test sets and lists of censored words. 10% and 30% of the keywords obtained from the 'train' function have been removed (\"censored\") from the messages in *test1* and *test2* respectively, therefore, I have no knowledge whether the test sets contain these words or not. The censored words, listed in the *.txt* files, must then be treated as missing values.\n",
    "\n",
    "I will modify the 'predict' function in the code to implement the change and use it to report accuracies on *test1*, with both 'train' and 'train2'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "3445ae7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NaiveBayesForSpamCensored(NaiveBayesForSpam):\n",
    "    def predictcensored(self, message, censored_words):\n",
    "        posterriors = np.copy(self.priors)\n",
    "        for i, w in enumerate(self.words):\n",
    "            if w in censored_words: # do nothing\n",
    "                continue\n",
    "            elif w in message.lower():\n",
    "                posterriors *= self.likelihoods[:, i]\n",
    "            else:\n",
    "                posterriors *= np.ones(2) - self.likelihoods[:, i]\n",
    "            posterriors = posterriors / np.linalg.norm(posterriors) # normalise\n",
    "        if posterriors[0] > 0.5:\n",
    "            return ['ham', posterriors[0]]\n",
    "        return ['spam', posterriors[1]]\n",
    "    \n",
    "    def scorecensored(self, messages, labels, censored_words):\n",
    "        confusion = np.zeros(4).reshape(2, 2)\n",
    "        for m, l in zip(messages, labels):\n",
    "            if self.predictcensored(m, censored_words)[0] == 'ham' and l == 'ham':\n",
    "                confusion[0, 0] += 1\n",
    "            elif self.predictcensored(m, censored_words)[0] == 'ham' and l == 'spam':\n",
    "                confusion[0, 1] += 1\n",
    "            elif self.predictcensored(m, censored_words)[0] == 'spam' and l == 'ham':\n",
    "                confusion[1, 0] += 1\n",
    "            elif self.predictcensored(m, censored_words)[0] == 'spam' and l == 'spam':\n",
    "                confusion[1, 1] += 1\n",
    "        return (confusion[0, 0] + confusion[1, 1]) / float(confusion.sum()), confusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "df56253a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For train function with censored words:\n",
      "Test1 accuracy is 0.9672897196261683\n",
      "Test1 confusion matrix is \n",
      "[[1101.   28.]\n",
      " [  14.  141.]]\n",
      "Run time: 109.8\n"
     ]
    }
   ],
   "source": [
    "clf_censored = NaiveBayesForSpamCensored()\n",
    "time1 = time.time()\n",
    "clf_censored.train(hammsgs, spammsgs)\n",
    "accuracy_censored_test1, confusion_matrix_censored_test1 = clf_censored.scorecensored(test1['sms'], test1['label'], cens1)\n",
    "time2 = time.time()\n",
    "print('For train function with censored words:')\n",
    "print('Test1 accuracy is {}'.format(accuracy_censored_test1))\n",
    "print('Test1 confusion matrix is \\n{}'.format(confusion_matrix_censored_test1))\n",
    "print(\"Run time: {}\".format(round(time2 - time1, 2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "c933909b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For train2 function with censored words:\n",
      "Test1 accuracy is 0.9688473520249221\n",
      "Test1 confusion matrix is \n",
      "[[1113.   38.]\n",
      " [   2.  131.]]\n",
      "Run time: 3.63\n"
     ]
    }
   ],
   "source": [
    "clf_censored2 = NaiveBayesForSpamCensored()\n",
    "time1 = time.time()\n",
    "clf_censored2.train2(hammsgs, spammsgs)\n",
    "accuracy_censored2_test1, confusion_matrix_censored2_test1 = clf_censored2.scorecensored(test1['sms'], test1['label'], cens1)\n",
    "time2 = time.time()\n",
    "print('For train2 function with censored words:')\n",
    "print('Test1 accuracy is {}'.format(accuracy_censored2_test1))\n",
    "print('Test1 confusion matrix is \\n{}'.format(confusion_matrix_censored2_test1))\n",
    "print(\"Run time: {}\".format(round(time2 - time1, 2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45a35a79",
   "metadata": {},
   "source": [
    "I repeat the process using test2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "85ce0393",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For train function with censored words:\n",
      "Test2 accuracy is 0.9611197511664075\n",
      "Test2 confusion matrix is \n",
      "[[1090.   46.]\n",
      " [   4.  146.]]\n",
      "Run time: 384.05\n"
     ]
    }
   ],
   "source": [
    "clf_censored = NaiveBayesForSpamCensored()\n",
    "time1 = time.time()\n",
    "clf_censored.train(hammsgs, spammsgs)\n",
    "accuracy_censored_test2, confusion_matrix_censored_test2 = clf_censored.scorecensored(test2['sms'], test2['label'], cens2)\n",
    "time2 = time.time()\n",
    "print('For train function with censored words:')\n",
    "print('Test2 accuracy is {}'.format(accuracy_censored_test2))\n",
    "print('Test2 confusion matrix is \\n{}'.format(confusion_matrix_censored_test2))\n",
    "print(\"Run time: {}\".format(round(time2 - time1, 2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "ec73a534",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For train2 function with censored words:\n",
      "Test2 accuracy is 0.9618973561430794\n",
      "Test2 confusion matrix is \n",
      "[[1092.   47.]\n",
      " [   2.  145.]]\n",
      "Run time: 15.15\n"
     ]
    }
   ],
   "source": [
    "clf_censored2 = NaiveBayesForSpamCensored()\n",
    "time1 = time.time()\n",
    "clf_censored2.train2(hammsgs, spammsgs)\n",
    "accuracy_censored2_test2, confusion_matrix_censored2_test2 = clf_censored2.scorecensored(test2['sms'], test2['label'], cens2)\n",
    "time2 = time.time()\n",
    "print('For train2 function with censored words:')\n",
    "print('Test2 accuracy is {}'.format(accuracy_censored2_test2))\n",
    "print('Test2 confusion matrix is \\n{}'.format(confusion_matrix_censored2_test2))\n",
    "print(\"Run time: {}\".format(round(time2 - time1, 2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7899f235",
   "metadata": {},
   "source": [
    "Performance gap between classifier trained using 'train2' and that using 'train' decreases for both *test1* and *test2* with respect to both accuracy and run time (from 20x to 17x).\n",
    "The result implies that randomly dropping some of the features may mitigate the violation of the conditional independence assumption and simplify the computation."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
